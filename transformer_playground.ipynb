{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self attention in depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In attention block we have three key factors.\n",
    "# query, key and value \n",
    "# to know what does query, key and value pls check Andrew Ng's transformer video\n",
    "# Now lets go through all the above in more depth\n",
    "# Example sentence 'what is self attention' this is having 4 words\n",
    "# lets take a particular word say \"attention\"\n",
    "# now \"attention\" can be represented as embedding vector  of any dimension, here we\n",
    "# are taking it as 24\n",
    "\n",
    "d_k = 24 #embedding dim\n",
    "seq_len = 4 # as we have an example sentence having 4 words\n",
    "x3 = np.random.randn(1, d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "# attention calculation for x3\n",
    "\n",
    "# We will only focus on x3\n",
    "# for x3 we need the query vector x3 \n",
    "\n",
    "head_dim = 3 # the dimension for query, key and  value\n",
    "wquery_3 = np.random.randn(d_k, head_dim)\n",
    "key = np.random.randn(seq_len, head_dim) # 4 is the vocab size here i.e. len(['what', 'is', 'self', 'attention'])\n",
    "value = np.random.randn(seq_len, head_dim)\n",
    "query_3 = np.dot(x3, wquery_3)  # query vector derived from x3 (1, 20)\n",
    "\n",
    "\n",
    "# In order to get the raw attention vale for x3 we need to \n",
    "# dot product of query3 with all the keys from other words\n",
    "\n",
    "score = []\n",
    "\n",
    "# we will collect the keys from each word to know the information it has and multiply it\n",
    "# with question we have \n",
    "# key roughly speaking is like answer so dot product of (question and answer) give you \n",
    "# amount information the previous or next word contains\n",
    "# values can be treated as reward of Reinfocement Learning\n",
    "\n",
    "# just to make it more intutive we name query_3 as question\n",
    "question = query_3\n",
    "for idx in range(seq_len):\n",
    "    answer = key[idx, :]\n",
    "    inforamtion_contains = np.dot(question, answer.T)\n",
    "    scaled_information_contains = inforamtion_contains / np.sqrt(d_k) #scaled by embedding dim\n",
    "    score.append(scaled_information_contains)\n",
    "\n",
    "score = np.array(score)\n",
    "print(score.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06819091]\n",
      " [-0.64732525]\n",
      " [-0.71929792]\n",
      " [ 0.14924954]]\n"
     ]
    }
   ],
   "source": [
    "# score is not scaled \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to scale the score we will use softmax function\n",
    "prob = np.exp(score) / np.sum(np.exp(score), axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30077514]\n",
      " [0.16854935]\n",
      " [0.15684467]\n",
      " [0.37383084]]\n"
     ]
    }
   ],
   "source": [
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16854935294368745\n"
     ]
    }
   ],
   "source": [
    "# prob[1] represents how much word3 should give attention to word2\n",
    "print(prob[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "[[ 0.06351373 -0.04478712  0.46063753]]\n"
     ]
    }
   ],
   "source": [
    "# final step\n",
    "# here we will aggregate the probs with reward\n",
    "A3 = np.zeros((1, head_dim))\n",
    "for idx in range(seq_len):\n",
    "    reward = value[idx, :]\n",
    "    A3 += prob[idx] * reward\n",
    "\n",
    "print(A3.shape)\n",
    "print(A3)\n",
    "\n",
    "# This is A3 is the single head attention value for the word \"X3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the above is only done for x3 which is attention \n",
    "# lets do it for all others\n",
    "\n",
    "x = np.random.randn(seq_len, d_k) # as 4 is the vocab size and 20 is embedding size\n",
    "w_query = np.random.rand(d_k, head_dim)\n",
    "w_key = np.random.rand(d_k, head_dim)\n",
    "w_value = np.random.randn(d_k, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "# first we will create an inefficient implementation of self attention\n",
    "\n",
    "A = [None for _ in range(seq_len)] # a list which will contain all attentions\n",
    "\n",
    "\n",
    "key = np.dot(x, w_key) # 4 is the vocab size here i.e. len(['what', 'is', 'self', 'attention'])\n",
    "value = np.dot(x, w_value)\n",
    "\n",
    "for idx in range(seq_len):\n",
    "    score = []\n",
    "    for j in range(seq_len):\n",
    "        query = np.dot(x[idx, :].reshape(1, -1), w_query)\n",
    "        answer = key[idx, :]\n",
    "        inforamtion_contains = np.dot(query, answer)\n",
    "        score.append(inforamtion_contains)\n",
    "    score = np.array(score)\n",
    "    prob = np.exp(score) / np.sum(np.exp(score), axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "    # final step\n",
    "    # here we will aggregate the probs with reward\n",
    "    attention = np.zeros((1, head_dim))\n",
    "    for k in range(seq_len):\n",
    "        reward = value[k, :]\n",
    "        attention += prob[k] * reward\n",
    "    A[idx] = attention\n",
    "\n",
    "A = np.array(A).reshape(seq_len, -1)\n",
    "print(A.shape)\n",
    "\n",
    "# Here A is containing all attention representaions of all the words in vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "# Now lets do all the above in a vectorized implementation\n",
    "x = np.random.rand(seq_len, d_k)\n",
    "\n",
    "w_query = np.random.rand(d_k, head_dim)\n",
    "w_key = np.random.rand(d_k, head_dim)\n",
    "w_value = np.random.randn(d_k, head_dim)\n",
    "\n",
    "\n",
    "query = np.dot(x, w_query)\n",
    "key = np.dot(x, w_key)\n",
    "value = np.dot(x, w_value)\n",
    "\n",
    "# Now instead of for loop, we will use np.dot for every next operations\n",
    "score = np.dot(query, key.T) \n",
    "probs = np.exp(score) / np.sum(np.exp(score), axis=0, keepdims=True)\n",
    "\n",
    "A = np.dot(probs, value)\n",
    "print(A.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "# Now for multi head attention \n",
    "# we just need to create multiple heads like this\n",
    "\n",
    "# lets say we want to create 8 heads inplace of 1 head \n",
    "\n",
    "\n",
    "multi_heads= []\n",
    "num_heads = 8    #this number is taken from the paper\n",
    "x = np.random.rand(seq_len, d_k)\n",
    "head_dim = d_k // num_heads\n",
    "\n",
    "\n",
    "\n",
    "for head in range(num_heads):\n",
    "    \n",
    "    w_query = np.random.rand(d_k, head_dim)\n",
    "    w_key = np.random.rand(d_k, head_dim)\n",
    "    w_value = np.random.randn(d_k, head_dim)\n",
    "\n",
    "\n",
    "    query = np.dot(x, w_query)\n",
    "    key = np.dot(x, w_key)\n",
    "    value = np.dot(x, w_value)\n",
    "\n",
    "    score = np.dot(query, key.T) \n",
    "    probs = np.exp(score) / np.sum(np.exp(score), axis=0, keepdims=True)\n",
    "\n",
    "    A = np.dot(probs, value)\n",
    "    multi_heads.append(A)\n",
    "\n",
    "multi_heads = np.array(multi_heads)\n",
    "\n",
    "# Now we can see, instead of one head we have 8 heads with much rich representation\n",
    "print(multi_heads.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
