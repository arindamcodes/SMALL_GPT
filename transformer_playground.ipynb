{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Self Attention In Depth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In attention block we have three key factors.\n",
    "# query, key and value \n",
    "# to know what does query, key and value pls check Andrew Ng's transformer video\n",
    "# Now lets go through all the above in more depth\n",
    "# Example sentence 'what is self attention' this is having 4 words\n",
    "# lets take a particular word say \"attention\"\n",
    "# now \"attention\" can be represented as embedding vector  of any dimension, here we\n",
    "# are taking it as 20\n",
    "\n",
    "x3 = np.random.randn(1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "# attention calculation for x3\n",
    "\n",
    "# We will only focus on x3\n",
    "# for x3 we need the query vector x3 \n",
    "wquery_3 = np.random.randn(20, 20)\n",
    "key = np.random.randn(4, 20) # 4 is the vocab size here i.e. len(['what', 'is', 'self', 'attention'])\n",
    "value = np.random.randn(4, 20)\n",
    "query_3 = np.dot(x3, wquery_3)  # query vector derived from x3 (1, 20)\n",
    "\n",
    "\n",
    "# In order to get the raw attention vale for x3 we need to \n",
    "# dot product of query3 with all the keys from other words\n",
    "\n",
    "score = []\n",
    "\n",
    "# we will collect the keys from each word to know the information it has and multiply it\n",
    "# with question we have \n",
    "# key roughly speaking is like answer so dot product of (question and answer) give you \n",
    "# amount information the previous or next word contains\n",
    "# values can be treated as reward of Reinfocement Learning\n",
    "\n",
    "# just to make it more intutive we name query_3 as question\n",
    "question = query_3\n",
    "for idx in range(4):\n",
    "    answer = key[idx, :]\n",
    "    inforamtion_contains = np.dot(question, answer.T)\n",
    "    score.append(inforamtion_contains)\n",
    "\n",
    "score = np.array(score)\n",
    "print(score.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.24002923]\n",
      " [-23.12175369]\n",
      " [-22.70992041]\n",
      " [-16.78562033]]\n"
     ]
    }
   ],
   "source": [
    "# score is not scaled \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to scale the score we will use softmax function\n",
    "prob = np.exp(score) / np.sum(np.exp(score), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.82107426e+02]\n",
      " [7.90399223e-22]\n",
      " [2.64721706e-22]\n",
      " [2.36825812e-13]]\n"
     ]
    }
   ],
   "source": [
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.903992231186865e-22\n"
     ]
    }
   ],
   "source": [
    "# prob[1] represents how much word3 should give attention to word2\n",
    "print(prob[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[  107.24998971  -735.93959195  -571.47619008    38.5024471\n",
      "    262.14588853  -175.60632075  -527.2970708   -329.55485186\n",
      "    108.84143762   -53.4805704    379.85877869   113.70570607\n",
      "   -139.24381018   410.97271662  -517.41667149   207.87093945\n",
      "   -105.60227675   362.96363259   200.23125176 -1498.35993269]]\n"
     ]
    }
   ],
   "source": [
    "# final step\n",
    "# here we will aggregate the probs with reward\n",
    "A3 = np.zeros((1, 20))\n",
    "for idx in range(4):\n",
    "    reward = value[idx, :]\n",
    "    A3 += prob[idx] * reward\n",
    "\n",
    "print(A3.shape)\n",
    "print(A3)\n",
    "\n",
    "# This is A3 is the single head attention value for the word \"X3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the above is only done for x3 which is attention \n",
    "# lets do it for all others\n",
    "\n",
    "x = np.random.randn(4, 20) # as 4 is the vocab size and 20 is embedding size\n",
    "w_query = np.random.rand(20, 20)\n",
    "w_key = np.random.rand(20, 20)\n",
    "w_value = np.random.randn(20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 20)\n"
     ]
    }
   ],
   "source": [
    "# first we will create an inefficient implementation of self attention\n",
    "\n",
    "A = [None for _ in range(4)] # a list which will contain all attentions\n",
    "\n",
    "\n",
    "key = np.dot(x, w_key) # 4 is the vocab size here i.e. len(['what', 'is', 'self', 'attention'])\n",
    "value = np.dot(x, w_value)\n",
    "\n",
    "vocab_size = 4 \n",
    "for idx in range(vocab_size):\n",
    "    score = []\n",
    "    for j in range(vocab_size):\n",
    "        query = np.dot(x[idx, :].reshape(1, -1), w_query)\n",
    "        answer = key[idx, :]\n",
    "        inforamtion_contains = np.dot(query, answer)\n",
    "        score.append(inforamtion_contains)\n",
    "    score = np.array(score)\n",
    "    prob = np.exp(score) / np.sum(np.exp(score), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "    # final step\n",
    "    # here we will aggregate the probs with reward\n",
    "    attention = np.zeros((1, 20))\n",
    "    for k in range(vocab_size):\n",
    "        reward = value[k, :]\n",
    "        attention += prob[k] * reward\n",
    "    A[idx] = attention\n",
    "\n",
    "A = np.array(A).reshape(vocab_size, -1)\n",
    "print(A.shape)\n",
    "\n",
    "# Here A is containing all attention representaions of all the words in vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 20)\n"
     ]
    }
   ],
   "source": [
    "# Now lets do all the above in a vectorized implementation\n",
    "x = np.random.rand(4, 20)\n",
    "\n",
    "w_query = np.random.rand(20, 20)\n",
    "w_key = np.random.rand(20, 20)\n",
    "w_value = np.random.randn(20, 20)\n",
    "\n",
    "\n",
    "query = np.dot(x, w_query)\n",
    "key = np.dot(x, w_key)\n",
    "value = np.dot(x, w_value)\n",
    "\n",
    "# Now instead of for loop, we will use np.dot for every next operations\n",
    "score = np.dot(query, key.T) \n",
    "probs = np.exp(score) / np.sum(np.exp(score), axis=1, keepdims=True)\n",
    "\n",
    "A = np.dot(probs, value)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
