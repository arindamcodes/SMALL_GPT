{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In attention block we have three key factors.\n",
    "# query, key and value \n",
    "# to know what does query, key and value pls check Andrew Ng's transformer video\n",
    "# Now lets go through all the above in more depth\n",
    "# lets take a particular word say \"attention\"\n",
    "# now \"attention\" can be represented as embedding vector  of any dimension, here we\n",
    "# are taking it as 20\n",
    "\n",
    "x3 = np.random.randn(1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "# attention calculation for x3\n",
    "\n",
    "# We will only focus on x3\n",
    "# for x3 we need the query vector x3 \n",
    "wquery_3 = np.random.randn(20, 20)\n",
    "key = np.random.randn(4, 20)\n",
    "value = np.random.randn(4, 20)\n",
    "query_3 = np.dot(x3, wquery_3)  # query vector derived from x3 (1, 20)\n",
    "\n",
    "\n",
    "# In order to get the raw attention vale for x3 we need to \n",
    "# dot product of query3 with all the keys from other words\n",
    "\n",
    "score = []\n",
    "\n",
    "# we will collect the keys from each word to know the information it has and multiply it\n",
    "# with question we have \n",
    "# key roughly speaking is like answer so dot product of (question and answer) give you \n",
    "# amount information the previous or next word contains\n",
    "# values can be treated as reward of Reinfocement Learning\n",
    "\n",
    "# just to make it more intutive we name query_3 as question\n",
    "question = query_3\n",
    "for idx in range(4):\n",
    "    answer = key[idx, :]\n",
    "    inforamtion_contains = np.dot(question, answer.T)\n",
    "    reward_gain = inforamtion_contains\n",
    "    score.append(reward_gain)\n",
    "\n",
    "score = np.array(score)\n",
    "print(score.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.24002923]\n",
      " [-23.12175369]\n",
      " [-22.70992041]\n",
      " [-16.78562033]]\n"
     ]
    }
   ],
   "source": [
    "# score is not scaled \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to scale the score we will use softmax function\n",
    "prob = np.exp(score) / np.sum(np.exp(scores), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.82107426e+02]\n",
      " [7.90399223e-22]\n",
      " [2.64721706e-22]\n",
      " [2.36825812e-13]]\n"
     ]
    }
   ],
   "source": [
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.903992231186865e-22\n"
     ]
    }
   ],
   "source": [
    "# prob[1] represents how much word3 should give attention to word2\n",
    "print(prob[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[  107.24998971  -735.93959195  -571.47619008    38.5024471\n",
      "    262.14588853  -175.60632075  -527.2970708   -329.55485186\n",
      "    108.84143762   -53.4805704    379.85877869   113.70570607\n",
      "   -139.24381018   410.97271662  -517.41667149   207.87093945\n",
      "   -105.60227675   362.96363259   200.23125176 -1498.35993269]\n",
      " [  107.24998971  -735.93959195  -571.47619008    38.5024471\n",
      "    262.14588853  -175.60632075  -527.2970708   -329.55485186\n",
      "    108.84143762   -53.4805704    379.85877869   113.70570607\n",
      "   -139.24381018   410.97271662  -517.41667149   207.87093945\n",
      "   -105.60227675   362.96363259   200.23125176 -1498.35993269]\n",
      " [  107.24998971  -735.93959195  -571.47619008    38.5024471\n",
      "    262.14588853  -175.60632075  -527.2970708   -329.55485186\n",
      "    108.84143762   -53.4805704    379.85877869   113.70570607\n",
      "   -139.24381018   410.97271662  -517.41667149   207.87093945\n",
      "   -105.60227675   362.96363259   200.23125176 -1498.35993269]\n",
      " [  107.24998971  -735.93959195  -571.47619008    38.5024471\n",
      "    262.14588853  -175.60632075  -527.2970708   -329.55485186\n",
      "    108.84143762   -53.4805704    379.85877869   113.70570607\n",
      "   -139.24381018   410.97271662  -517.41667149   207.87093945\n",
      "   -105.60227675   362.96363259   200.23125176 -1498.35993269]]\n"
     ]
    }
   ],
   "source": [
    "# final step\n",
    "# here we will aggregate the probs with reward\n",
    "\n",
    "A3 = np.dot(prob.T, value)\n",
    "print(A3.shape)\n",
    "\n",
    "A3 = np.zeros((4, 20))\n",
    "for idx in range(4):\n",
    "    reward = value[idx, :]\n",
    "    A3 += prob[idx] * reward\n",
    "\n",
    "print(A3)\n",
    "\n",
    "# This is A3 is the single head attention value for the word \"X3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
